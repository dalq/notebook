## 多元线性回归
- d(J(w))/dw0 = 1/m∑[h(w)-y]
- d(J(w))/dw1 = 1/m∑[h(w)-y]x
- 多元的梯度下降法：d(J(w))/dwj = 1/m∑[h(w)-y]xj（上边第一个公式0是x0，上边第二个公式x是x1）

### 梯度下降
- 特征缩放：以二元回归为例，如果俩特征量级相差比较大的话，代价函数就会很细长（偏移严重），这样可能需要很多次下降才能达到最优点
    + 比如将几个特征缩放到接近[-1, 1]的范围内，x0 = 1
- 归一化：x1:= (x1-u1)/s1：u1-特征x1的均值，s1-x1的范围--x1的最大值减去x1的最小值
- 学习速率𝞪：如果J(θ)是逐渐上升的、抛物线、周期的抛物线，说明需要使用一个小的𝞪；下降缓慢（相比较比较陡的下降）可能是𝞪太小了，从较小的选取，不断取大的来测试
- 如果代价函数 J(θ) 的下降小于 一个很小的值 ε 那么就认为已经收敛 比如可以选择 1e-3 

### 特征选取、多项式回归
- 二次函数最后会下降，平方根的话是不会下降的

### 正规方程法
- 计算梯度下降，迭代算法需要多次迭代计算来收敛到全局最小点
- 平方误差函数：$J(\theta_1, \theta_2, ..., \theta_m) = \frac 1 2 \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$   --> d(J(θ))/dθj = 0  --> 
- $\theta = (X^TX)^{-1}X^Ty$
- 不需要特征缩放，但是如果用梯度下降法的话就需要特征缩放
- 不需要选择学习速率，不需要迭代
- 计算逆矩阵的计算量大概是维度的3次方：O (kn2)  ；  O (n3), need to calculate inverse of XTX
- 特征太大的化需要考虑使用梯度下降法了
-  $x^TX$不可逆的原因：
    + 1、如果有冗余的特征的话（利于$x_1 = 1.5x_2$），可以去掉它以减少
    + 2、如果特征维度大于样本未读，考虑去掉一些特征；或者使用regularization
