# essence
- pattern
- can't pin it down machincal
- use data to solve it

## 1、定义
- task T：识别邮件是否是垃圾的
- experience E：观察你标记（过）的邮件是否是垃圾的
- performance measure P：正确标记的垃圾邮件个数

## 监督学习
- 部分数据集已有答案（训练集）
- 回归问题：预测连续值输出
- 分类：预测离散值输出，输出可能不止2个的
- 支持向量机：处理无线多的特征

## 无监督学习
- 判断是否有不通的聚类（结构）：news.google.com
    - 组织计算机集群、社交网络分析、客户市场分类、天文数据
- 分离不同的声音：SVD

## 回归模型
根据之前的数据预测出准确的输出值


## 2、单变量线性回归
- x1开始
- 如何得到hypothesis？
- **hypothesis、parameters、cost function、goal**

#### 代价函数
- h(x)=w0 + w1 * x ：w0和w1称为模型参数
- 尽量让h(x)很好的与数据点(训练集)拟合
- h(x)尽可能的接近y
- 标准定义(最小化过程)：minimize(w0, w1) {∑[h(x_i) - y_i]^2}/2m：尽量减少平均误差
- 找到训练集中预测值和真实值的差的平方的和的1/2m最小的w0和w1的值
- minimize(w0, w1) J(w0, w1)->代价函数、平方误差函数

#### 梯度下降算法
- 不停地改变w0、w1，视图通过这种改变使得J(w0, w1)变小
- learning rate：学习速率
- 同步更新w0和w1
- **学习速率、当前位置的导数，会影响下一步的大小**越接近最优解，越平缓的化，整体的步长会越来越小（导数越来越趋近于0）
- d(J(w))/dw0 = 1/m∑[h(w)-y]
- d(J(w))/dw1 = 1/m∑[h(w)-y]x
- 批梯度下降法：如果数据集足够充分，那么用一半、甚至少得多的数据训练出来的梯度与用全部训练数据训练出来的梯度几乎是一样的；
- 二元线性回归：没有局部最优-是全局最优的

#### 线性代数复习
- 矩阵的维度：行个数*列个数
- 向量是nx1的
- scala：标量
- subtraction：减法
- prediction = data_matrix * parameter_matrix
    + 结果的第n列是第n组参数下得到的预测值
- 不能交换律但是可以结合律
- 矩阵求逆：没有逆矩阵的矩阵->奇异矩阵、退化矩阵
- 矩阵转置：
